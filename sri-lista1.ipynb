{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista 1 : Recuperação de informação "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import machado, mac_morpho\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "\n",
    "import string\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gabi_28_gabi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package machado to\n",
      "[nltk_data]     /Users/gabi_28_gabi/nltk_data...\n",
      "[nltk_data]   Package machado is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('machado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = []\n",
    "for i in machado.fileids():\n",
    "    textos.append(machado.raw(i))\n",
    "\n",
    "swu = stopwords.words('portuguese') + list (string.punctuation)\n",
    "stemmer = PortugueseStemmer()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sem stemming\n",
    "textos_normal = []\n",
    "for texto in textos:\n",
    "    tlimpo = [token.lower() for token in WordPunctTokenizer().tokenize(texto) if token not in swu]\n",
    "    textos_normal.append(tlimpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com stemming\n",
    "textos_stem = []\n",
    "for texto in textos_normal:\n",
    "    tlimpo = [stemmer.stem(token) for token in texto]\n",
    "    textos_stem.append(tlimpo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conto', 'contos', 'fluminenses', '1870', 'contos', 'fluminenses', 'texto', 'fonte', 'obra', 'completa']\n",
      "['cont', 'cont', 'fluminens', '1870', 'cont', 'fluminens', 'text', 'font', 'obra', 'complet']\n"
     ]
    }
   ],
   "source": [
    "print(textos_normal[0][0:10])\n",
    "print(textos_stem[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_normal = defaultdict(lambda:set([]))\n",
    "for tid,t in enumerate(textos_normal):\n",
    "    for term in t:\n",
    "        indice_normal[term].add(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_stem = defaultdict(lambda:set([]))\n",
    "for tid,t in enumerate(textos_stem):\n",
    "    for term in t:\n",
    "        indice_stem[term].add(tid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca(consulta, indice):\n",
    "    \n",
    "    tokens = WordPunctTokenizer().tokenize(consulta)\n",
    "\n",
    "    resultado = set()\n",
    "    operador = 'or'\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == 'and':\n",
    "            operador = 'and'\n",
    "            \n",
    "        elif token == 'or':\n",
    "            operador = 'or'\n",
    "        \n",
    "        elif token == 'not':\n",
    "            operador = 'not'\n",
    "            \n",
    "        else:\n",
    "            resultado_temp = indice[token]\n",
    "            if operador == 'and':\n",
    "                resultado = resultado & resultado_temp\n",
    "            \n",
    "            elif operador == 'not':\n",
    "                resultado = resultado - resultado_temp\n",
    "                \n",
    "            else:\n",
    "                resultado = resultado | resultado_temp\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1 \n",
    "\n",
    "##### Baseando-se no indice invertido construído na prática 1, Calcule a diferença de revocação com e sem a utilização de \"stemming\", ou truncagem na construção do índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreRev(recuperados, relevantes):\n",
    "    \n",
    "    # precisao \n",
    "    pre = len(recuperados &  relevantes)/len(recuperados)\n",
    "    # revocacao\n",
    "    rev = len(recuperados &  relevantes)/len(relevantes)\n",
    "    \n",
    "    return pre,rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavra a ser consultada\n",
    "palavra = 'contos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Resultados Stemmizado: 219\n",
      "Quantidade de Resultados Não Stemmizado: 74\n"
     ]
    }
   ],
   "source": [
    "# Resultados da busca\n",
    "res_normal = busca(palavra,indice_normal)\n",
    "res_stem = busca(stemmer.stem(palavra), indice_stem)\n",
    "\n",
    "print('Quantidade de Resultados Stemmizado: ' + str(len(res_stem)))\n",
    "\n",
    "print('Quantidade de Resultados Não Stemmizado: ' + str(len(res_normal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão: 1.0\n",
      "Revocação: 0.3378995433789954\n"
     ]
    }
   ],
   "source": [
    "print('Precisão: ' +  str(PreRev(res_normal, res_stem)[0]))\n",
    "print('Revocação: ' + str(PreRev(res_normal, res_stem)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2 \n",
    "\n",
    "#### Crie grupos de equivalência para alguns termos de busca e calcule a diferença em termos de revocação e, possivelmente precisão, na resposta a consultas expandidas e não expandidas. Dica: use tempos verbais, pluralização, sinônimos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "termos = ['contos', 'conto', 'contar', 'contando', 'narração', 'narrações'] \n",
    "#termos = ['ator', 'atores','atriz','atrizes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos encontrados sem Stemming:\n",
      " contos: 74\n",
      " conto: 166\n",
      " contar: 128\n",
      " contando: 58\n",
      " narração: 84\n",
      " narrações: 13\n",
      "\n",
      "\n",
      "TOTAL: 201\n"
     ]
    }
   ],
   "source": [
    "res_normal2 = set()\n",
    "\n",
    "print('Documentos encontrados sem Stemming:')\n",
    "for termo in termos:\n",
    "    res = busca(termo, indice_normal)\n",
    "    res_normal2.update(res)\n",
    "    print(' {}: {}'.format(termo, len(res)))\n",
    "print('\\n')\n",
    "print('TOTAL: {}'.format(len(res_normal2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos encontrados sem Stemming:\n",
      " contos: 219\n",
      " conto: 219\n",
      " contar: 219\n",
      " contando: 219\n",
      " narração: 84\n",
      " narrações: 13\n",
      "\n",
      "\n",
      "TOTAL: 221\n"
     ]
    }
   ],
   "source": [
    "res_stem2 = set()\n",
    "\n",
    "print('Documentos encontrados sem Stemming:')\n",
    "for termo in termos:\n",
    "    res = busca(stemmer.stem(termo), indice_stem)\n",
    "    res_stem2.update(res)\n",
    "    print(' {}: {}'.format(termo, len(res)))\n",
    "print('\\n')\n",
    "print('TOTAL: {}'.format(len(res_stem2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão: 1.0\n",
      "Revocação: 0.9095022624434389\n"
     ]
    }
   ],
   "source": [
    "print('Precisão: ' +  str(PreRev(res_normal2, res_stem2)[0]))\n",
    "print('Revocação: ' + str(PreRev(res_normal2, res_stem2)[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3 \n",
    "\n",
    "#### Implemente uma expansão de consulta por meio da correção ortográfica. Utilize o corretor ortográfico Pyenchant para fazer as correções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicionario\n",
    "d = enchant.Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrige_palavra(palavra):\n",
    "    \n",
    "    if not d.check(palavra):\n",
    "        \n",
    "        print('Você quis dizer: ')\n",
    "        for p in d.suggest(palavra)[:5]:\n",
    "            print('-', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrige_palavra('ontos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_correta(palavra,ind,dic):\n",
    "\n",
    "    # faz a verificação ortográfica e a busca sobre os termos possíveis\n",
    "    \n",
    "    if not dic.check(palavra):\n",
    "        resultado = set()\n",
    "        for token in dic.suggest(palavra):\n",
    "            resultado = resultado | ind[token]\n",
    "    else:\n",
    "        resultado = ind[termo]\n",
    "        \n",
    "    return resultado\n",
    "\n",
    "def busca2(consulta, indice):\n",
    "    \n",
    "    d = enchant.Dict(\"pt_BR\")\n",
    "    tokens = WordPunctTokenizer().tokenize(consulta)\n",
    "\n",
    "    resultado = set()\n",
    "    operador = 'or'\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == 'and':\n",
    "            operador = 'and'\n",
    "            \n",
    "        elif token == 'or':\n",
    "            operador = 'or'\n",
    "        \n",
    "        elif token == 'not':\n",
    "            operador = 'not'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #resultado_temp = indice[token]\n",
    "            # Caso um termo esteja escrito errado, faz-se a busca sobre todas as correções sugeridas\n",
    "            resultado_temp = busca_correta(token, indice, d)\n",
    "            \n",
    "            if operador == 'and':\n",
    "                resultado = resultado & resultado_temp\n",
    "            \n",
    "            elif operador == 'not':\n",
    "                resultado = resultado - resultado_temp\n",
    "                \n",
    "            else:\n",
    "                resultado = resultado | resultado_temp\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavra a ser consultada\n",
    "palavra = 'ontos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Resultados Stemmizado: 0\n",
      "Quantidade de Resultados Não Stemmizado: 0\n"
     ]
    }
   ],
   "source": [
    "# Resultados da busca usando a função criada para os exercícios anteriores (busca)\n",
    "res_normal = busca(palavra,indice_normal)\n",
    "res_stem = busca(stemmer.stem(palavra), indice_stem)\n",
    "\n",
    "print('Quantidade de Resultados Stemmizado: ' + str(len(res_stem)))\n",
    "\n",
    "print('Quantidade de Resultados Não Stemmizado: ' + str(len(res_normal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados da busca usando a função adaptada que usa a correção ortográfica (busca2)\n",
    "res_normal = busca2(palavra,indice_normal)\n",
    "res_stem = busca2(stemmer.stem(palavra), indice_stem)\n",
    "\n",
    "print('Quantidade de Resultados Stemmizado: ' + str(len(res_stem)))\n",
    "\n",
    "print('Quantidade de Resultados Não Stemmizado: ' + str(len(res_normal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4 \n",
    "\n",
    "#### Implemente um indice invertido que permita consulta por frases, conforme definido na aula 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_limpos = []\n",
    "for texto in textos:\n",
    "    tlimpo = [stemmer.stem(token.lower()) for token in WordPunctTokenizer().tokenize(texto) if token not in swu]\n",
    "    textos_limpos.append(tlimpo)\n",
    "    \n",
    "indice_frase = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for texto_id, texto in enumerate(textos_limpos):\n",
    "    for token_id, token in enumerate(texto):\n",
    "        indice_frase[token][texto_id].append(token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([230])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indice_frase[stemmer.stem(\"Capitu\")].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indice_frase[stemmer.stem(\"Capitu\")][230])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximidade_termos(lista1, lista2, distancia):\n",
    "    resultado = []\n",
    "    \n",
    "    for i in lista1:\n",
    "        for j in lista2:\n",
    "            if abs(i-j) <= distancia:\n",
    "                resultado.append(round((i+j)/2))\n",
    "                \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_frase(frase, indice, distancia=1):\n",
    "    \n",
    "    tokens = [stemmer.stem(token.lower()) for token in WordPunctTokenizer().tokenize(frase) if token not in swu]\n",
    "    \n",
    "    # Verificar quais documentos possuem todos os termos\n",
    "    matches = indice[tokens[0]].keys()\n",
    "    \n",
    "    for token in tokens[1:]:\n",
    "        matches = matches & indice[token].keys()    \n",
    "        \n",
    "    # Comparar as posições dentro dos documentos em comum\n",
    "    resultado = defaultdict(list)\n",
    "    \n",
    "    for documento in matches:\n",
    "        \n",
    "        resultado_temp = indice[tokens[0]][documento]\n",
    "        \n",
    "        for token in tokens[1:]:\n",
    "            resultado_temp = proximidade_termos(resultado_temp,indice[token][documento],distancia)\n",
    "            \n",
    "        if resultado_temp:\n",
    "            resultado[documento] = resultado_temp\n",
    "            \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_frase(frase, indice):\n",
    "    tokens = [stemmer.stem(token.lower()) for token in WordPunctTokenizer().tokenize(frase) if token not in swu]\n",
    " \n",
    "    matches = defaultdict(lambda:defaultdict(lambda:set([])))\n",
    "    for token in tokens:\n",
    "        matches[token] = indice[token]\n",
    "    \n",
    "    doc_pos = set(matches[tokens[0]])\n",
    "    for tok in tokens:\n",
    "        doc_pos = doc_pos.intersection(set(matches[token]))\n",
    "        \n",
    "    resultado = set([]) \n",
    "    for doc in doc_pos:\n",
    "        for pos in matches[tokens[0]][doc]:\n",
    "            if all((pos + token_id + 1 in matches[token][doc]) for token_id, token in enumerate(tokens[1:len(tokens)])):\n",
    "                resultado.add(doc) \n",
    "                \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_frase(frase, indice):\n",
    "    tokens = [stemmer.stem(token.lower()) for token in WordPunctTokenizer().tokenize(frase) if token not in swu]\n",
    " \n",
    "    matches = defaultdict(lambda:defaultdict(lambda:set([])))\n",
    "    for token in tokens:\n",
    "        matches[token] = indice[token]\n",
    "    \n",
    "    doc_pos = set(matches[tokens[0]])\n",
    "    for tok in tokens:\n",
    "        doc_pos = doc_pos.intersection(set(matches[token]))\n",
    "        \n",
    "    resultado = set([]) \n",
    "    for doc in doc_pos:\n",
    "        for pos in matches[tokens[0]][doc]:\n",
    "            if all((pos + token_id + 1 in matches[token][doc]) for token_id, token in enumerate(tokens[1:len(tokens)])):\n",
    "                resultado.add(doc) \n",
    "                \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{230}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busca_frase(\"olhos de cigana oblíqua e dissimulada\", indice_frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5 \n",
    "\n",
    "#### Modifique a solução acima para permitir respostas alternativas caso a frase não retorne resultados. Por exemplo, retornar, documentos que contenham parte da frase, ou uma busca booleana simples combinando as palavras da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whoosh\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh import qparser\n",
    "from whoosh.qparser import QueryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(content=TEXT(phrase=True, stored=True))\n",
    "\n",
    "if os.path.exists('indexdir'):\n",
    "    ix = open_dir('indexdir')\n",
    "else:\n",
    "    os.mkdir('indexdir')\n",
    "    ix = create_in(\"indexdir\", schema)\n",
    "    writer = ix.writer()\n",
    "    for txt in textos:\n",
    "        writer.add_document(content=txt)\n",
    "    writer.commit()\n",
    "\n",
    "\n",
    "searcher = ix.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_frase_alt(frase, indice):\n",
    "        \n",
    "    tokens = [stemmer.stem(token.lower()) for token in WordPunctTokenizer().tokenize(frase) if token not in swu]\n",
    " \n",
    "    resultado = busca_frase(frase, indice)\n",
    "                \n",
    "    if resultado == set([]):\n",
    "        searcher = ix.searcher()\n",
    "        query = QueryParser(\"content\", ix.schema).parse(frase)\n",
    "        resultado_alt = searcher.search(query)\n",
    "        return resultado_alt.docs()\n",
    "    else:\n",
    "        return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{230}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busca_frase_alt(\"olhos de cigana dissimulada e oblíqua\", indice_frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{82, 193, 224, 230, 244}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busca_frase_alt(\"ressaca de olhos\", indice_frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
